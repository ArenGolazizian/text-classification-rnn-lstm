{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5ZbzcSP60pK"
   },
   "source": [
    "# Text Classification\n",
    "\n",
    "## RNN & LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI5BRkcqEvSe"
   },
   "source": [
    "In this project, I will implement, train, and evaluate Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) models on a text classification task using a dataset of IMDB movie reviews, and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kQ3EFONl9Cd",
    "outputId": "66e2fdd6-e253-45db-b928-141fe1eeed05"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import wordpunct_tokenize\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.core.display import display, HTML\n",
    "tqdm.pandas()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NKWaZBn8NMY"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "In this section, weâ€™ll load the IMDB dataset and preprocess the data to make it suitable for training RNN and LSTM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_9UAw1I8uvP"
   },
   "source": [
    "## Load Dataset\n",
    "Description of Dataset: The IMDB movie reviews dataset consists of reviews along with their labels (positive or negative sentiment). Each review is a sentence or paragraph of text.\n",
    "\n",
    "Download the Dataset: We will use a Google Drive link to download the dataset into our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s463eDIjmGmc",
    "outputId": "28130fa4-d4a2-454a-c541-8d045a08f1d3"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/imdb_reviews.csv'\n",
    "# gdd.download_file_from_google_drive(file_id='1zfM5E6HvKIe7f3rEt1V2gBpw5QOSSKQz',dest_path=DATA_PATH,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apPnMJrH9AG4"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "For our models to work effectively, we need to preprocess the text data by cleaning it and converting words to integer indices for training.Preproces steps\n",
    "such as Tokenization and Cleaning , Replacing Rare Words , Build Vocabulary , Convert Tokens to Indices and Prepare Data for Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8egndGf-GIR"
   },
   "outputs": [],
   "source": [
    "def tokenize(text, stop_words):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNlaFTd-mJuS"
   },
   "outputs": [],
   "source": [
    "def remove_rare_words(tokens, common_tokens, max_len):\n",
    "    return [token if token in common_tokens\n",
    "            else '<UNK>' for token in tokens][-max_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsJ38mMjmObb"
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(data_path, max_vocab, max_len):\n",
    "    df = pd.read_csv(data_path)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    df['tokens'] = df['review'].apply(lambda x: tokenize(x, stop_words))\n",
    "\n",
    "    all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
    "    common_tokens = set(list(zip(*Counter(all_tokens).most_common(max_vocab)))[0])\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: remove_rare_words(x, common_tokens, max_len))\n",
    "\n",
    "    df = df[df['tokens'].apply(lambda tokens: any(token != '<UNK>' for token in tokens))]\n",
    "\n",
    "    vocab = sorted(set([token for tokens in df['tokens'] for token in tokens]))\n",
    "    token2idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "    token2idx['<PAD>'] = len(token2idx)\n",
    "\n",
    "    df['indexed_tokens'] = df['tokens'].apply(lambda tokens: [token2idx[token] for token in tokens])\n",
    "\n",
    "    return df['indexed_tokens'].tolist(), df['label'].tolist(), token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S86p_iDJmRbs"
   },
   "outputs": [],
   "source": [
    "max_vocab = 2500\n",
    "\n",
    "max_len = 100\n",
    "\n",
    "sequences, targets, token2idx = load_and_preprocess_data(DATA_PATH, max_vocab, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hq_1Kc9Vmqp2"
   },
   "outputs": [],
   "source": [
    "def split_data(sequences, targets, valid_ratio=0.05, test_ratio=0.05):\n",
    "    total_size = len(sequences)\n",
    "    test_size = int(total_size * test_ratio)\n",
    "    valid_size = int(total_size * valid_ratio)\n",
    "    train_size = total_size - valid_size - test_size\n",
    "\n",
    "    train_sequences, train_targets = sequences[:train_size], targets[:train_size]\n",
    "    valid_sequences, valid_targets = sequences[train_size:train_size + valid_size], targets[train_size:train_size + valid_size]\n",
    "    test_sequences, test_targets = sequences[train_size + valid_size:], targets[train_size + valid_size:]\n",
    "\n",
    "    return train_sequences, train_targets, valid_sequences, valid_targets, test_sequences, test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rX0ZC4acmsaV"
   },
   "outputs": [],
   "source": [
    "train_sequences, train_targets, valid_sequences, valid_targets, test_sequences, test_targets = split_data(sequences, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoS4k2wwmzaN"
   },
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs_padded = pad_sequences(inputs, padding_val=token2idx['<PAD>'])\n",
    "    return torch.LongTensor(inputs_padded), torch.LongTensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_29NGAJnDo8"
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_val=0, pad_left=False):\n",
    "    \"\"\"Pad a list of sequences to the same length with a padding_val.\"\"\"\n",
    "    sequence_length = max(len(sequence) for sequence in sequences)\n",
    "    if not pad_left:\n",
    "        return [sequence + [padding_val] * (sequence_length - len(sequence)) for sequence in sequences]\n",
    "    return [[padding_val] * (sequence_length - len(sequence)) + sequence for sequence in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HB33EytF1QCW"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_data = list(zip(train_sequences, train_targets))\n",
    "valid_data = list(zip(valid_sequences, valid_targets))\n",
    "test_data = list(zip(test_sequences, test_targets))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjwZOOd6_E1J"
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EB2IPhH_fDR"
   },
   "source": [
    "## RNN with nn.RNN\n",
    "Implement a basic RNN model using PyTorch's built-in nn.RNN.\n",
    "\n",
    "Layers: embedding, RNN, and fully connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KR4ggfh8nK3d"
   },
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, vocab_size,\n",
    "                 device, n_layers=1,\n",
    "                 embedding_dimension=50):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dimension, padding_idx=token2idx['<PAD>'])\n",
    "        self.rnn = nn.RNN(input_size=embedding_dimension,\n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers=n_layers,\n",
    "                          batch_first=True,\n",
    "                          nonlinearity='tanh')  \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embedding(inputs) \n",
    "        output, hidden = self.rnn(embedded)  \n",
    "        out = self.fc(hidden[-1]) \n",
    "        return out  # logits for each class in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Xv9SWFfFPjA"
   },
   "source": [
    "### Train model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2orLI-qXeFW"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    model.to(device)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        train_loss = epoch_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(valid_loader, desc=f\"Validation Epoch {epoch}\"):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "                val_total += targets.size(0)\n",
    "\n",
    "        val_loss = val_loss / val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFw6Wl7xFW7m",
    "outputId": "aa5f1624-b52d-4336-f637-a68597003753"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "output_size = 2  # Positive or Negative\n",
    "hidden_size = 128  # Hyperparameter to be adjusted\n",
    "vocab_size = len(token2idx)\n",
    "n_layers = 1\n",
    "embedding_dimension = 50\n",
    "\n",
    "rnn_model = RNNClassifier(output_size=output_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          vocab_size=vocab_size,\n",
    "                          device=device,\n",
    "                          n_layers=n_layers,\n",
    "                          embedding_dimension=embedding_dimension)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "rnn_model = train_model(rnn_model, train_loader, valid_loader, criterion, optimizer, device, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJpjgTNc_rdx"
   },
   "source": [
    "## RNN from Scratch\n",
    "Implement an RNN from scratch by creating a custom RNN cell and a model that stacks these cells over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjJfWY4zfsBR"
   },
   "outputs": [],
   "source": [
    "class CustomRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomRNNCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input2hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden2hidden = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        new_hidden = torch.tanh(self.input2hidden(input) + self.hidden2hidden(hidden))\n",
    "        return new_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPODDCcAft1a"
   },
   "outputs": [],
   "source": [
    "class CustomRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=token2idx['<PAD>'])\n",
    "        self.rnn_cell = CustomRNNCell(input_size=embedding_dim, hidden_size=hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embedding(inputs)  \n",
    "        batch_size, seq_length, embedding_dim = embedded.size()\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size).to(inputs.device)\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            hidden = self.rnn_cell(embedded[:, t, :], hidden)\n",
    "\n",
    "        out = self.fc(hidden) \n",
    "        return out  # logits for each class in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMTPh0---Y63"
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRgR4YxHfv5w",
    "outputId": "f8f4f317-602e-4fea-f0b0-15c0ba3a33b7"
   },
   "outputs": [],
   "source": [
    "output_size = 2 \n",
    "hidden_size = 128 \n",
    "vocab_size = len(token2idx)\n",
    "n_layers = 1\n",
    "embedding_dimension = 50\n",
    "\n",
    "custom_rnn_model = CustomRNN(vocab_size=vocab_size,\n",
    "                             embedding_dim=embedding_dimension,\n",
    "                             hidden_size=hidden_size,\n",
    "                             output_size=output_size)\n",
    "\n",
    "criterion_custom_rnn = nn.CrossEntropyLoss()\n",
    "optimizer_custom_rnn = optim.Adam(custom_rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "custom_rnn_model = train_model(custom_rnn_model, train_loader, valid_loader, criterion_custom_rnn, optimizer_custom_rnn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tfvjr-8ECg4"
   },
   "source": [
    "### Evaluate RNN models on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZkeBWtgFbXA"
   },
   "outputs": [],
   "source": [
    "def evaluate_on_test(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true_test = []\n",
    "    y_pred_test = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true_test.extend(targets.cpu().numpy())\n",
    "            y_pred_test.extend(predicted.cpu().numpy())\n",
    "    print(classification_report(y_true_test, y_pred_test, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JkpP3q-xGotx",
    "outputId": "ae505698-0598-484c-cafa-fd84363c86c1"
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating Built-in RNN Model on Test Set:\")\n",
    "evaluate_on_test(rnn_model, test_loader)\n",
    "\n",
    "print(\"\\nEvaluating Custom RNN Model on Test Set:\")\n",
    "evaluate_on_test(custom_rnn_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwlqjNQkAEBL"
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwvNPJsoAFUf"
   },
   "source": [
    "## LSTM with nn.LSTM\n",
    "Define an LSTM model using PyTorch's built-in nn.LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "id": "r2lPsPSc-ZQP",
    "outputId": "00e70258-bc2e-400c-8ba2-0f481acf45d7"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, vocab_size,\n",
    "                 device, bidirectional=False, n_layers=1,\n",
    "                 embedding_dimension=50):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dimension, padding_idx=token2idx['<PAD>'])\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dimension,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=n_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * self.num_directions, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h0 = torch.zeros(self.n_layers * self.num_directions, inputs.size(0), self.hidden_size).to(inputs.device)\n",
    "        c0 = torch.zeros(self.n_layers * self.num_directions, inputs.size(0), self.hidden_size).to(inputs.device)\n",
    "\n",
    "        embedded = self.embedding(inputs)  \n",
    "        lstm_out, (hn, cn) = self.lstm(embedded, (h0, c0))  \n",
    "        if self.bidirectional:\n",
    "            hn = hn.view(self.n_layers, self.num_directions, inputs.size(0), self.hidden_size)\n",
    "            hn_forward = hn[-1, 0, :, :]  \n",
    "            hn_backward = hn[-1, 1, :, :] \n",
    "            hn_combined = torch.cat((hn_forward, hn_backward), dim=1)  \n",
    "            out = self.fc(hn_combined)\n",
    "        else:\n",
    "            out = self.fc(hn[-1])  \n",
    "        return out  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gh3-B63fFgra"
   },
   "source": [
    "### Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnYI5bAAXhqg"
   },
   "outputs": [],
   "source": [
    "lstm_model = LSTMClassifier(output_size=output_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            vocab_size=vocab_size,\n",
    "                            device=device,\n",
    "                            bidirectional=False,\n",
    "                            n_layers=n_layers,\n",
    "                            embedding_dimension=embedding_dimension)\n",
    "\n",
    "criterion_lstm = nn.CrossEntropyLoss()\n",
    "optimizer_lstm = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "lstm_model = train_model(lstm_model, train_loader, valid_loader, criterion_lstm, optimizer_lstm, device, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvLAiujIAOWh"
   },
   "source": [
    "## Custom LSTM from Scratch\n",
    "Implement an LSTM from scratch by defining a LSTM cell and a model that combines these cells over the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JyccBL7zvwGZ"
   },
   "outputs": [],
   "source": [
    "class CustomLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomLSTMCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.output_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.cell_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell_state):\n",
    "        combined = torch.cat((input, hidden), dim=1)   \n",
    "        i = torch.sigmoid(self.input_gate(combined))   \n",
    "        f = torch.sigmoid(self.forget_gate(combined))   \n",
    "        o = torch.sigmoid(self.output_gate(combined))  \n",
    "        g = torch.tanh(self.cell_gate(combined))   \n",
    "        cell_state = f * cell_state + i * g\n",
    "        hidden = o * torch.tanh(cell_state)\n",
    "        return hidden, cell_state  # New hidden state , New cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88b8uj3Gv1JB"
   },
   "outputs": [],
   "source": [
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
    "        super(CustomLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=token2idx['<PAD>'])\n",
    "        self.lstm_cell = CustomLSTMCell(input_size=embedding_dim, hidden_size=hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size).to(inputs.device)\n",
    "        cell_state = torch.zeros(batch_size, self.hidden_size).to(inputs.device)\n",
    "        embedded = self.embedding(inputs)  \n",
    "        batch_size, seq_length, embedding_dim = embedded.size()\n",
    "        for t in range(seq_length):\n",
    "            hidden, cell_state = self.lstm_cell(embedded[:, t, :], hidden, cell_state)\n",
    "        \n",
    "        out = self.fc(hidden) \n",
    "        return out  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nx6vjFS-FnxW"
   },
   "source": [
    "### Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBNu0nfHXjsB"
   },
   "outputs": [],
   "source": [
    "custom_lstm_model = CustomLSTM(vocab_size=vocab_size,\n",
    "                               embedding_dim=embedding_dimension,\n",
    "                               hidden_size=hidden_size,\n",
    "                               output_size=output_size)\n",
    "\n",
    "criterion_custom_lstm = nn.CrossEntropyLoss()\n",
    "optimizer_custom_lstm = optim.Adam(custom_lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "custom_lstm_model = train_model(custom_lstm_model, train_loader, valid_loader, criterion_custom_lstm, optimizer_custom_lstm, device, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYv7jyi4F2j5"
   },
   "source": [
    "### Evaluate LSTM models on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0Yyousdi7ps"
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating Built-in LSTM Model on Test Set:\")\n",
    "evaluate_on_test(lstm_model, test_loader)\n",
    "\n",
    "print(\"\\nEvaluating Custom LSTM Model on Test Set:\")\n",
    "evaluate_on_test(custom_lstm_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iML4dcBmAc95"
   },
   "source": [
    "## Testing RNN and LSTM Models on a New Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B14PDnHiCMzH"
   },
   "outputs": [],
   "source": [
    "# Example review\n",
    "review = \"It is no wonder that the film has such a high rating, it is quite literally breathtaking. What can I say that hasn't said before? Not much, it's the story, the acting, the premise, but most of all, this movie is about how it makes you feel. Sometimes you watch a film, and can't remember it days later, this film loves with you, once you've seen it, you don't forget.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hql6MyMCK6_"
   },
   "source": [
    "### Preprocess the test Review\n",
    "To prepare the review for the model, we need to follow similar preprocessing steps as we did for the dataset:\n",
    "\n",
    "Remove special characters and convert the text to lowercase.\n",
    "\n",
    "Tokenize the text into individual words.\n",
    "\n",
    "Remove stopwords to focus only on meaningful words.\n",
    "\n",
    "Convert tokens to indices based on the token2idx dictionary created earlier.\n",
    "\n",
    "Pad or truncate the sequence to a length of max_len .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKpDw77QBC84"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_text(text, stop_words, token2idx, max_len):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens_idx = [token2idx.get(token, token2idx['<UNK>']) for token in tokens]\n",
    "    if len(tokens_idx) < max_len:\n",
    "        tokens_idx = tokens_idx + [token2idx['<PAD>']] * (max_len - len(tokens_idx))\n",
    "    else:\n",
    "        tokens_idx = tokens_idx[:max_len]\n",
    "    return tokens_idx\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "review_indices = preprocess_text(review, stop_words, token2idx, max_len)\n",
    "print(f\"Processed review indices: {review_indices}\")\n",
    "input_tensor = torch.LongTensor([review_indices]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqvvQrhjCSNY"
   },
   "source": [
    "### Make Predictions\n",
    "Now that we have preprocessed the review, use both the RNN and LSTM models to make predictions on the sentiment of the review.\n",
    "\n",
    "Set the model to evaluation mode to prevent updates during inference.\n",
    "Predict the sentiment class by passing the input_tensor to the model.\n",
    "Interpret the prediction as either \"Positive\" or \"Negative\" based on the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LtJcRwdCfyK"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(model, input_tensor, model_name=\"Model\"):\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        outputs = model(input_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        class_label = \"Positive\" if predicted.item() == 1 else \"Negative\"\n",
    "    print(f\"The predicted class for the review by {model_name} is: {class_label}\")\n",
    "\n",
    "predict_sentiment(rnn_model, input_tensor, model_name=\"Built-in RNN\")\n",
    "predict_sentiment(custom_rnn_model, input_tensor, model_name=\"Custom RNN\")\n",
    "predict_sentiment(lstm_model, input_tensor, model_name=\"Built-in LSTM\")\n",
    "predict_sentiment(custom_lstm_model, input_tensor, model_name=\"Custom LSTM\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
